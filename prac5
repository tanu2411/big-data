practical 5 code


1.      
Create a new folder logged in
as Hadoop user or where you have installed spark


2.      
Create file
‘NetworkWordCount.scala’ in that folder


import org.apache.spark.SparkConf


import org.apache.spark.streaming._


object NetworkWordCount {


  def main(args:
Array[String]): Unit = {


    val sparkConf
= new
SparkConf().setMaster("local[2]")setAppName("NetworkWordCount")


    val ssc = new
StreamingContext(sparkConf, Seconds(10))


    val lines =
ssc.socketTextStream("localhost",9999)


    val words =
lines.flatMap(_.split(" "))


    val tuples =
words.map(word => (word ,1))


    val wordCounts
= tuples.reduceByKey((t, v) => t + v)


   
wordCounts.print()


    ssc.start()


   
ssc.awaitTermination()


  }


}


3.      
Create a ‘networkwordcount.sbt’
file in the same folder


name := "networkwordcount"


version := "1.0.0"


scalaVersion := "2.12.15"


libraryDependencies +=
"org.apache.spark" % "spark-streaming_2.12" %
"3.3.1" % "provided"


 


4.      
Create the sbt package


$ sbt package


 


5.      
Start the spark server


$ start-master.sh


$ start-worker.sh spark://Ubuntu.myguest.virtualbox.org:7077/


6.      
On a separate terminal, start
the netscape server


$ nc –lk 9999


7.      
On the original terminal,
submit the scala program to spark


$ spark-submit –class ‘NetworkWordCount –master ‘spark://Ubuntu.myguest.virtualbox.org:7077/’
/target/scala-2.12/networkwordcount_2.12-1.0.0.jar


8.      
On the netscape terminal
provide textual input for the scala program to count words of the live stream
every 10 seconds
